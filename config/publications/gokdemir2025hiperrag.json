{
    "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights",
    "authors": [
        "Ozan Gokdemir",
        "Carlo Siebenschuh",
        "Alexander Brace",
        "Azton Wells",
        "Brian Hsu",
        "Kyle Hippe",
        "Priyanka V. Setty",
        "Aswathy Ajith",
        "J. Gregory Pauloski",
        "Varuni Sastry",
        "Sam Foreman",
        "Huihuo Zheng",
        "Heng Ma",
        "Bharat Kale",
        "Nicholas Chia",
        "Thomas Gibbs",
        "Michael E. Papka",
        "Thomas Brettin",
        "Francis J. Alexander",
        "Anima Anandkumar",
        "Ian Foster",
        "Rick Stevens",
        "Venkatram Vishwanath",
        "Arvind Ramanathan"
    ],
    "venue": "PASC25",
    "tldr": "The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. We introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4.",
    "paper": "https://arxiv.org/pdf/2505.04846",
    "awards": null,
    "bibtex": "gokdemir2025hiperrag",
    "code": "https://github.com/ramanathanlab/distllm",
    "website": null,
    "poster": null,
    "slides": null,
    "preprint": "https://arxiv.org/abs/2505.04846",
    "publisher": null,
    "year": 2025,
    "month": 5,
    "selected": false,
    "category": "science"
}
