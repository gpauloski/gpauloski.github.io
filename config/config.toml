[build]
build_dir = "./_site"
templates_dir = "./templates"
static_dir = "./static"

[overview]
name = "Greg Pauloski"
titles = ["Computer Scientist", "Software Engineer"]
headshot = "images/headshot.jpg"
contacts = [
    { email = "mailto:jgpauloski@uchicago.edu" },
    { github = "https://github.com/gpauloski" },
    { linkedin = "https://www.linkedin.com/in/gregorypauloski/" },
    { scholar = "https://scholar.google.com/citations?user=rek2K4EAAAAJ&hl=en" },
]
source = "https://github.com/gpauloski/gpauloski.github.io/"
text = """
Hello there!
I am a third-year Ph.D. student in Computer Science at the <a href="https://cs.uchicago.edu/" target="_blank">University of Chicago</a> interested in high-performance computing and machine learning systems.
I am a member of <a href="https://labs.globus.org" target="_blank">Globus Labs</a> where I am co-advised by <a href="https://cs.uchicago.edu/people/ian-foster/" target="_blank">Ian Foster</a> and <a href="https://kylechard.com/" target="_blank">Kyle Chard</a>.
I completed my Bachelors in Computer Science at the <a href="https://cs.utexas.edu/" target="_blank">University of Texas at Austin</a> and previously worked at Apple, Google, and the Texas Advanced Computing Center.
"""

[research]
"Scalable Deep Learning" = """\
We are exploring new techniques for improving deep learning training time and scalability by (1) exploiting scalable algorithms for second-order information approximation; (2) developing methods for adapting to different computer hardware by tuning computation and communication to maximize training speed; and (3) exploring compression techniques to reduce communication overheads.
"""
"Workflow Systems" = """\
Modern computational science experiments are increasingly written as a coupled set of many distinct software coordinated by a central workflow system.
We are designing new programming models which decouple communication from application design to enable multiple data movement methods depending on where data are moved, what are moved, or when they are moved.
"""
"Scientific Language Models" = """\
We are building large (billion+ parameter) transformer-based language models on broad scientific literature to automate knowledge extraction.
We are evaluating the training methods for these models to quantify the impact of training corpus size, model size, and pretraining time on downstream performance, and we are investigating better methods for assessing the quality of the trained models.
"""

[projects]
github = "https://github.com/gpauloski/"
links = [
    { "ProxyStore" = "https://github.com/proxystore" },
    { "Distributed KFAC Preconditioner" = "https://github.com/gpauloski/kfac-pytorch" },
    { "Language Model Training" = "https://github.com/gpauloski/llm-pytorch" },
    { "Colmena" = "https://github.com/exalearn/colmena" },
    { "3pseatBot: a Discord Bot" = "https://github.com/gpauloski/3pseatBot" },
]

[publications]
bibtex = "publications/pauloski.bib"
publications_dir = "./config/publications"

[presentations]
presentations_dir = "./config/presentations"
