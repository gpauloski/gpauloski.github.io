<!DOCTYPE html>

<div class="bullet">
    <div>
        <i class="material-icons">chevron_right</i>
    </div>
    <div id="bullet-entry">
        <b>Scalable Deep Learning:</b>
        We are exploring new techniques for improving deep learning training time and scalability by
        (1) exploiting scalable algorithms for second-order information approximation;
        (2) developing methods for adapting to different computer hardware by tuning computation and communication to maximize training speed; and
        (3) exploring compression techniques to reduce communication overheads.
    </div>
</div>

<div class="bullet">
    <div>
        <i class="material-icons">chevron_right</i>
    </div>
    <div id="bullet-entry">
        <b>Workflow Systems:</b>
        Modern computational science experiments are increasingly written as a coupled set of many distinct software coordinated by a central workflow system.
        We are designing new programming models which decouple communication from application design to enable multiple data movement methods depending on where data are moved, what are moved, or when they are moved.
    </div>
</div>

<div class="bullet">
    <div>
        <i class="material-icons">chevron_right</i>
    </div>
    <div id="bullet-entry">
        <b>Scientific Language Models:</b>
        We are building large (billion+ parameter) transformer-based language models on broad scientific literature to automate knowledge extraction.
        We are evaluating the training methods for these models to quantify the impact of training corpus size, model size, and pretraining time on downstream performance, and we are investigating better methods for assessing the quality of the trained models.
    </div>
</div>
