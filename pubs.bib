@ARTICLE{hong2021moleculesnlp,
	AUTHOR={Hong, Zhi and Pauloski, J. Gregory and Ward, Logan and Chard, Kyle and Blaiszik, Ben and Foster, Ian},
	TITLE={Models and Processes to Extract Drug-like Molecules From Natural Language Text},
	JOURNAL={Frontiers in Molecular Biosciences},
	VOLUME={8},
	PAGES={826},
	YEAR={2021},
	URL={https://www.frontiersin.org/article/10.3389/fmolb.2021.636077},
	DOI={10.3389/fmolb.2021.636077},
	ISSN={2296-889X},
	ABSTRACT={Researchers worldwide are seeking to repurpose existing drugs or discover new drugs to counter the disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies is molecules that have been reported in the scientific literature to be drug-like in the context of viral research. However, this literature is too large for human review and features unusual vocabularies for which existing named entity recognition (NER) models are ineffective. We report here on a project that leverages both human and artificial intelligence to detect references to such molecules in free text. We present 1) a iterative model-in-the-loop method that makes judicious use of scarce human expertise in generating training data for a NER model, and 2) the application and evaluation of this method to the problem of identifying drug-like molecules in the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198,875 papers. We show that by repeatedly presenting human labelers only with samples for which an evolving NER model is uncertain, our human-machine hybrid pipeline requires only modest amounts of non-expert human labeling time (tens of hours to label 1778 samples) to generate an NER model with an F-1 score of 80.5%—on par with that of non-expert humans—and when applied to CORD’19, identifies 10,912 putative drug-like molecules. This enriched the computational screening team’s targets by 3,591 molecules, of which 18 ranked in the top 0.1% of all 6.6 million molecules screened for docking against the 3CLPro protein.}
}

@misc{pauloski2021kaisa,
      title={KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural Networks}, 
      author={J. Gregory Pauloski and Qi Huang and Lei Huang and Shivaram Venkataraman and Kyle Chard and Ian Foster and Zhao Zhang},
      year={2021},
      eprint={2107.01739},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{pauloski2020kfac,
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	title = {Convolutional Neural Network Training with Distributed K-FAC},
	year = {2020},
	isbn = {9781728199986},
	publisher = {IEEE Press},
	abstract = {Training neural networks with many processors can reduce time-to-solution; however, it is challenging to maintain convergence and efficiency at large scales. The Kronecker-factored Approximate Curvature (K-FAC) was recently proposed as an approximation of the Fisher Information Matrix that can be used in natural gradient optimizers. We investigate here a scalable K-FAC design and its applicability in convolutional neural network (CNN) training at scale. We study optimization techniques such as layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling to reduce training time while preserving convergence. We use residual neural networks (ResNet) applied to the CIFAR-10 and ImageNet-1k datasets to evaluate the correctness and scalability of our K-FAC gradient preconditioner. With ResNet-50 on the ImageNet-1k dataset, our distributed K-FAC implementation converges to the 75.9% MLPerf baseline in 18--25% less time than does the classic stochastic gradient descent (SGD) optimizer across scales on a GPU cluster.},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	articleno = {94},
	numpages = {14},
	keywords = {optimization methods, neural networks, high performance computing, scalability},
	location = {Atlanta, Georgia},
	series = {SC '20},
	doi = {10.5555/3433701.3433826}
}

@inproceedings{zhang2020compressed,
	author={Z. {Zhang} and L. {Huang} and J. G. {Pauloski} and I. T. {Foster}},
	booktitle={2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
	title={Efficient I/O for Neural Network Training with Compressed Data}, 
	year={2020},
	volume={},
	number={},
	pages={409-418},
	doi={10.1109/IPDPS47924.2020.00050}
}

@INPROCEEDINGS{zhang2019aggregating,
	author={Z. {Zhang} and L. {Huang} and J. G. {Pauloski} and I. {Foster}},
	booktitle={2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS)}, 
	title={Aggregating Local Storage for Scalable Deep Learning I/O}, 
	year={2019},
	volume={},
	number={},
	pages={69-75},
	doi={10.1109/DLS49591.2019.00014}
}

@InProceedings{10.1007/978-3-030-11726-9_42,
	author="Gates, Evan
	and Pauloski, J. Gregory
	and Schellingerhout, Dawid
	and Fuentes, David",
	editor="Crimi, Alessandro
	and Bakas, Spyridon
	and Kuijf, Hugo
	and Keyvan, Farahani
	and Reyes, Mauricio
	and van Walsum, Theo",
	title="Glioma Segmentation and a Simple Accurate Model for Overall Survival Prediction",
	booktitle="Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",
	year="2019",
	publisher="Springer International Publishing",
	address="Cham",
	pages="476--484",
	abstract="Brain tumor segmentation is a challenging task necessary for quantitative tumor analysis and diagnosis. We apply a multi-scale convolutional neural network based on the DeepMedic to segment glioma subvolumes provided in the 2018 MICCAI Brain Tumor Segmentation Challenge. We go on to extract intensity and shape features from the images and cross-validate machine learning models to predict overall survival. Using only the mean FLAIR intensity, nonenhancing tumor volume, and patient age we are able to predict patient overall survival with reasonable accuracy.",
	isbn="978-3-030-11726-9"
}
